/*****************************************************************************
 * Copyright (C) 2013 x265 project
 *
 * Authors: Deepthi Devaki <deepthidevaki@multicorewareinc.com>,
 *          Rajesh Paulraj <rajesh@multicorewareinc.com>
 *          Mandar Gurav <mandar@multicorewareinc.com>
 *          Mahesh Pittala <mahesh@multicorewareinc.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
 *
 * This program is also available under a commercial proprietary license.
 * For more information, contact us at licensing@multicorewareinc.com.
 *****************************************************************************/

#define IF_INTERNAL_PREC 14 ///< Number of bits for internal precision
#define IF_FILTER_PREC    6 ///< Log2 of sum of filter taps
#define IF_INTERNAL_OFFS (1 << (IF_INTERNAL_PREC - 1)) ///< Offset used internally

template<int N>
void filterVertical_s_p(int bitDepth, short *src, int srcStride,
                        pixel *dst, int dstStride, int block_width, int block_height, short const *coeff)
{
    int row, col;

    src -= (N / 2 - 1) * srcStride;
    int offset;
    short maxVal;
    int headRoom = IF_INTERNAL_PREC - bitDepth;
    int shift = IF_FILTER_PREC;
    shift += headRoom;
    offset = 1 << (shift - 1);
    offset +=  IF_INTERNAL_OFFS << IF_FILTER_PREC;
    maxVal = (1 << bitDepth) - 1;
    Vec4i cm0(coeff[0]), cm1(coeff[1]), cm2(coeff[2]), cm3(coeff[3]), cm4(coeff[4]), cm5(coeff[5]), cm6(coeff[6]), cm7(coeff[7]);
    Vec16uc sum_uc;
    Vec8s vec_zero(0);

    for (row = 0; row < block_height; row++)
    {
        for (col = 0; col < block_width - 7; col += 8)
        {
            Vec8s row0, row1, row2, row3, row4, row5, row6, row7, sum;
            Vec4i row0_first, row0_last, row1_first, row1_last, sum_first, sum_last;
            Vec4i c0, c1, c2, c3, c4, c5, c6, c7;

            row0.load(&src[col]);
            row1.load(&src[col + srcStride]);

            c0 = cm0;
            c1 = cm1;

            row0_first = extend_low(row0);
            row1_first = extend_low(row1);
            row0_last = extend_high(row0);
            row1_last = extend_high(row1);

            row0_first = row0_first * c0;
            row1_first = row1_first * c1;
            row0_last = row0_last * c0;
            row1_last = row1_last * c1;

            sum_first = row0_first + row1_first;
            sum_last = row0_last + row1_last;

            row2.load(&src[col + 2 * srcStride]);
            row3.load(&src[col + 3 * srcStride]);

            c2 = cm2;
            c3 = cm3;

            row0_first = extend_low(row2);
            row0_last = extend_high(row2);
            row0_first = row0_first * c2;
            row0_last = row0_last * c2;
            row1_first = extend_low(row3);
            row1_last = extend_high(row3);
            row1_first = row1_first * c3;
            row1_last = row1_last * c3;
            sum_first += row0_first + row1_first;
            sum_last += row0_last + row1_last;

            if (N == 8)
            {
                row4.load(&src[col + 4 * srcStride]);
                row5.load(&src[col + 5 * srcStride]);

                c4 = cm4;
                c5 = cm5;

                row0_first = extend_low(row4);
                row0_last = extend_high(row4);
                row0_first = row0_first * c4;
                row0_last = row0_last * c4;
                row1_first = extend_low(row5);
                row1_last = extend_high(row5);
                row1_first = row1_first * c5;
                row1_last = row1_last * c5;
                sum_first += row0_first + row1_first;
                sum_last += row0_last + row1_last;

                row6.load(&src[col + 6 * srcStride]);
                row7.load(&src[col + 7 * srcStride]);

                c6 = cm6;
                c7 = cm7;

                row0_first = extend_low(row6);
                row0_last = extend_high(row6);
                row0_first = row0_first * c6;
                row0_last = row0_last * c6;
                row1_first = extend_low(row7);
                row1_last = extend_high(row7);
                row1_first = row1_first * c7;
                row1_last = row1_last * c7;
                sum_first += row0_first + row1_first;
                sum_last += row0_last + row1_last;
            }
            sum_first = (sum_first + offset)  >> shift;
            sum_last = (sum_last + offset)  >> shift;
            Vec4i zero(0);
            sum = compress(sum_first, sum_last);
            sum = max(sum, 0);
            Vec8s maxVal_v(maxVal);
            sum = min(sum, maxVal_v);
            sum_uc = compress(sum, vec_zero);
            sum_uc.store_partial(8, dst + col);
        }

        //Handle the case when block_width is not multiple of 8
        for (; col < block_width; col += 4)
        {
            Vec8s row0, row1, row2, row3, row4, row5, row6, row7, sum;
            Vec4i row0_first, row0_last, row1_first, row1_last, sum_first, sum_last;
            Vec4i c0, c1, c2, c3, c4, c5, c6, c7;

            row0.load(&src[col]);
            row1.load(&src[col + srcStride]);

            c0 = cm0;
            c1 = cm1;

            row0_first = extend_low(row0);
            row1_first = extend_low(row1);
            row0_first = row0_first * c0;
            row1_first = row1_first * c1;

            sum_first = row0_first + row1_first;

            row2.load(&src[col + 2 * srcStride]);
            row3.load(&src[col + 3 * srcStride]);

            c2 = cm2;
            c3 = cm3;

            row0_first = extend_low(row2);
            row0_first = row0_first * c2;
            row1_first = extend_low(row3);
            row1_first = row1_first * c3;
            sum_first += row0_first + row1_first;
            if (N == 8)
            {
                row4.load(&src[col + 4 * srcStride]);
                row5.load(&src[col + 5 * srcStride]);

                c4 = cm4;
                c5 = cm5;

                row0_first = extend_low(row4);
                row0_first = row0_first * c4;
                row1_first = extend_low(row5);
                row1_first = row1_first * c5;
                sum_first += row0_first + row1_first;

                row6.load(&src[col + 6 * srcStride]);
                row7.load(&src[col + 7 * srcStride]);

                c6 = cm6;
                c7 = cm7;

                row0_first = extend_low(row6);
                row0_first = row0_first * c6;
                row1_first = extend_low(row7);
                row1_first = row1_first * c7;
                sum_first += row0_first + row1_first;
            }
            sum_first = (sum_first + offset)  >> shift;
            Vec4i zero(0);
            sum = compress(sum_first, zero);
            sum = max(sum, 0);
            Vec8s maxVal_v(maxVal);
            sum = min(sum, maxVal_v);
            sum_uc = compress(sum, vec_zero);
            sum_uc.store_partial(block_width - col, dst + col);
        }

        src += srcStride;
        dst += dstStride;
    }
}

/*
    Please refer Fig 7 in HEVC Overview document to familiarize with variables' naming convention
    Input: Subpel from the Horizontal filter - 'src'
    Output: All planes in the corresponding column - 'dst<A|E|I|P>'
*/

#if INSTRSET >= 5
#define PROCESSROW(a0, a1, a2, a3, a4, a5, a6, a7) { \
    tmp = _mm_loadu_si128((__m128i const*)(src + col + (row + 7) * srcStride)); \
    a7 = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15)); \
    exp1 = _mm_sub_epi32(_mm_sub_epi32(_mm_sll_epi32(a1, _mm_cvtsi32_si128(2)), a0), _mm_mullo_epi32(a2, _mm_set1_epi32(10))); \
    exp2 = _mm_mullo_epi32(a3, _mm_set1_epi32(40)); \
    exp3 = _mm_mullo_epi32(a3, _mm_set1_epi32(17)); \
    exp4 = _mm_mullo_epi32(a4, _mm_set1_epi32(17)); \
    exp5 = _mm_mullo_epi32(a4, _mm_set1_epi32(40)); \
    exp6 = _mm_sub_epi32(_mm_sub_epi32(_mm_sll_epi32(a6, _mm_cvtsi32_si128(2)), a7), _mm_mullo_epi32(a5, _mm_set1_epi32(10))); \
    sume = _mm_add_epi32(exp1, _mm_add_epi32(_mm_add_epi32(exp2, exp3), \
                                        _mm_add_epi32(_mm_add_epi32(a3, exp4), \
                                                        _mm_add_epi32(_mm_mullo_epi32(a5, _mm_set1_epi32(-5)), \
                                                                    a6) \
                                                        ) \
                                        ) \
                            ); \
    sumi = _mm_sub_epi32(_mm_add_epi32(_mm_add_epi32(exp1, exp2), _mm_add_epi32(exp5, exp6)), \
                            _mm_add_epi32(a2, a5)); \
    sump = _mm_add_epi32(a1, _mm_add_epi32(_mm_add_epi32(exp3, exp4), \
                                        _mm_add_epi32(_mm_add_epi32(exp5, exp6), \
                                                        _mm_add_epi32(_mm_mullo_epi32(a2, _mm_set1_epi32(-5)), \
                                                                    a4) \
                                                        ) \
                                        ) \
                            ); \
    /* store results */ \
    sumi = _mm_sra_epi32(_mm_add_epi32(sumi, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sumi, _mm_setzero_si128()); \
    sumi = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstI + row * dstStride + col) = _mm_cvtsi128_si32(sumi); \
    sume = _mm_sra_epi32(_mm_add_epi32(sume, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sume, _mm_setzero_si128()); \
    sume = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstE + row * dstStride + col) = _mm_cvtsi128_si32(sume); \
    sump = _mm_sra_epi32(_mm_add_epi32(sump, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sump, _mm_setzero_si128()); \
    sump = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstP + row * dstStride + col) = _mm_cvtsi128_si32(sump); \
}
#else /* if INSTRSET >= 5 */
#define PROCESSROW(a0, a1, a2, a3, a4, a5, a6, a7) { \
    tmp = _mm_loadu_si128((__m128i const*)(src + col + (row + 7) * srcStride)); \
    a7 = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15)); \
    /*  calculation

        The coefficients for different planes are :
        e:    { -1, 4, -10, 58, 17,  -5, 1,  0 },
        i:    { -1, 4, -11, 40, 40, -11, 4, -1 },
        p:    {  0, 1,  -5, 17, 58, -10, 4, -1 }
        Thus the expressions are:
        sume = 4*a1 -a0 - 10*a2 + 58*a3 + 17*a4 -  5*a5 +   a6     ;
        sumi = 4*a1 -a0 - 11*a2 + 40*a3 + 40*a4 - 11*a5 + 4*a6  -a7;
        sump =   a1      - 5*a2 + 17*a3 + 58*a4 - 10*a5 + 4*a6  -a7;
        */\
    exp1 = (a1 << 2) - a0 - 10 * a2; \
    exp2 = 40 * a3; \
    exp3 = 17 * a3; \
    exp4 = 17 * a4; \
    exp5 = 40 * a4; \
    exp6 = (a6 << 2) - a7 - 10 * a5; \
    sume = exp1 + exp2 + exp3 + a3 + exp4 - 5 * a5 +   a6; \
    sumi = exp1 - a2 + exp2 + exp5 + exp6 -   a5; \
    sump = a1 - 5 * a2 + exp3 + exp4 + exp5 + a4 + exp6; \
    /* store results */ \
    sumi = _mm_sra_epi32(_mm_add_epi32(sumi, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sumi, _mm_setzero_si128()); \
    sumi = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstI + row * dstStride + col) = _mm_cvtsi128_si32(sumi); \
    sume = _mm_sra_epi32(_mm_add_epi32(sume, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sume, _mm_setzero_si128()); \
    sume = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstE + row * dstStride + col) = _mm_cvtsi128_si32(sume); \
    sump = _mm_sra_epi32(_mm_add_epi32(sump, _mm_set1_epi32(offset)), _mm_cvtsi32_si128(12)); \
    tmp  =  _mm_packs_epi32(sump, _mm_setzero_si128()); \
    sump = _mm_packus_epi16(tmp, _mm_setzero_si128()); \
    *(uint32_t*)(dstP + row * dstStride + col) = _mm_cvtsi128_si32(sump); \
    }
#endif /* if INSTRSET >= 5 */

#if INSTRSET >= 5
#define EXTENDCOL(X, Y) { /*X=0 for leftmost column, X=block_width+marginX for rightmost column*/ \
        tmp16e = _mm_shuffle_epi8(sume, _mm_set1_epi8(Y)); \
        tmp16i = _mm_shuffle_epi8(sumi, _mm_set1_epi8(Y)); \
        tmp16p = _mm_shuffle_epi8(sump, _mm_set1_epi8(Y)); \
        for (int i = -marginX; i < -16; i += 16) \
        { \
            _mm_storeu_si128((__m128i*)(dstE + row * dstStride + X + i), tmp16e); \
            _mm_storeu_si128((__m128i*)(dstI + row * dstStride + X + i), tmp16i); \
            _mm_storeu_si128((__m128i*)(dstP + row * dstStride + X + i), tmp16p); \
        } \
        _mm_storeu_si128((__m128i*)(dstE + row * dstStride + X - 16), tmp16e); /*Assuming marginX > 16*/ \
        _mm_storeu_si128((__m128i*)(dstI + row * dstStride + X - 16), tmp16i); \
        _mm_storeu_si128((__m128i*)(dstP + row * dstStride + X - 16), tmp16p); \
}
#else /* if INSTRSET >= 5 */
#define EXTENDCOL(X, Y) { /*X=0 for leftmost column, X=block_width+marginX for rightmost column*/ \
        tmp16e = permute16uc<Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y>((Vec16uc)sume); \
        tmp16i = permute16uc<Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y>((Vec16uc)sumi); \
        tmp16p = permute16uc<Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y>((Vec16uc)sump); \
        for (int i = -marginX; i < -16; i += 16) \
        { \
            tmp16e.store(dstE + row * dstStride + X + i); \
            tmp16i.store(dstI + row * dstStride + X + i); \
            tmp16p.store(dstP + row * dstStride + X + i); \
        } \
        tmp16e.store(dstE + row * dstStride + X - 16);      /*Assuming marginX > 16*/ \
        tmp16i.store(dstI + row * dstStride + X - 16);      \
        tmp16p.store(dstP + row * dstStride + X - 16);      \
}
#endif /* if INSTRSET >= 5 */

void filterVerticalMultiplaneExtend(int /*bitDepth*/,
                                    short *src, int srcStride,
                                    pixel *dstE, pixel *dstI, pixel *dstP, int dstStride,
                                    int block_width, int block_height,
                                    int marginX, int marginY)
{
    int row, col;
    int offset;
    int headRoom = IF_INTERNAL_PREC - 8;
    int shift = IF_FILTER_PREC + headRoom;

    offset = 1 << (shift - 1);
    offset +=  IF_INTERNAL_OFFS << IF_FILTER_PREC;
    src -= (8 / 2 - 1) * srcStride;

#if INSTRSET < 5
    Vec16uc tmp16e, tmp16i, tmp16p;
    Vec4i a0, a1, a2, a3, a4, a5, a6, a7, sum;
    Vec8s tmp;
    Vec4i val, sume, sumi, sump;
    Vec4i exp1, exp2, exp3, exp4, exp5, exp6;
#else
    __m128i tmp16e, tmp16i, tmp16p;
    __m128i a0, a1, a2, a3, a4, a5, a6, a7;
    __m128i tmp;
    __m128i sume, sumi, sump;
    __m128i exp1, exp2, exp3, exp4, exp5, exp6;
#endif /* if INSTRSET < 5 */

    col = 0;

    tmp = _mm_loadu_si128((__m128i const*)(src + col));
    a0  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + srcStride));
    a1  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 2 * srcStride));
    a2  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 3 * srcStride));
    a3  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 4 * srcStride));
    a4  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 5 * srcStride));
    a5  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 6 * srcStride));
    a6  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));

    for (row = 0; row < block_height; row++)
    {
        PROCESSROW(a0, a1, a2, a3, a4, a5, a6, a7) EXTENDCOL(0, 0) row++;
        PROCESSROW(a1, a2, a3, a4, a5, a6, a7, a0) EXTENDCOL(0, 0) row++;
        PROCESSROW(a2, a3, a4, a5, a6, a7, a0, a1) EXTENDCOL(0, 0) row++;
        PROCESSROW(a3, a4, a5, a6, a7, a0, a1, a2) EXTENDCOL(0, 0) row++;
        PROCESSROW(a4, a5, a6, a7, a0, a1, a2, a3) EXTENDCOL(0, 0) row++;
        PROCESSROW(a5, a6, a7, a0, a1, a2, a3, a4) EXTENDCOL(0, 0) row++;
        PROCESSROW(a6, a7, a0, a1, a2, a3, a4, a5) EXTENDCOL(0, 0) row++;
        PROCESSROW(a7, a0, a1, a2, a3, a4, a5, a6) EXTENDCOL(0, 0)
    }

    col += 4;

    for ( /*col = 0*/; col < block_width - 4; col += 4)         // Considering block width is always a multiple of 4
    {
        tmp = _mm_loadu_si128((__m128i const*)(src + col));
        a0  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + srcStride));
        a1  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + 2 * srcStride));
        a2  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + 3 * srcStride));
        a3  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + 4 * srcStride));
        a4  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + 5 * srcStride));
        a5  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
        tmp = _mm_loadu_si128((__m128i const*)(src + col + 6 * srcStride));
        a6  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));

        for (row = 0; row < block_height; row++)
        {
            PROCESSROW(a0, a1, a2, a3, a4, a5, a6, a7) row++;
            PROCESSROW(a1, a2, a3, a4, a5, a6, a7, a0) row++;
            PROCESSROW(a2, a3, a4, a5, a6, a7, a0, a1) row++;
            PROCESSROW(a3, a4, a5, a6, a7, a0, a1, a2) row++;
            PROCESSROW(a4, a5, a6, a7, a0, a1, a2, a3) row++;
            PROCESSROW(a5, a6, a7, a0, a1, a2, a3, a4) row++;
            PROCESSROW(a6, a7, a0, a1, a2, a3, a4, a5) row++;
            PROCESSROW(a7, a0, a1, a2, a3, a4, a5, a6)
        }
    }

    tmp = _mm_loadu_si128((__m128i const*)(src + col));
    a0  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + srcStride));
    a1  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 2 * srcStride));
    a2  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 3 * srcStride));
    a3  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 4 * srcStride));
    a4  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 5 * srcStride));
    a5  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));
    tmp = _mm_loadu_si128((__m128i const*)(src + col + 6 * srcStride));
    a6  = _mm_unpacklo_epi16(tmp, _mm_srai_epi16(tmp, 15));

    for (row = 0; row < block_height; row++)
    {
        PROCESSROW(a0, a1, a2, a3, a4, a5, a6, a7) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a1, a2, a3, a4, a5, a6, a7, a0) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a2, a3, a4, a5, a6, a7, a0, a1) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a3, a4, a5, a6, a7, a0, a1, a2) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a4, a5, a6, a7, a0, a1, a2, a3) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a5, a6, a7, a0, a1, a2, a3, a4) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a6, a7, a0, a1, a2, a3, a4, a5) EXTENDCOL((block_width + marginX), 3) row++;
        PROCESSROW(a7, a0, a1, a2, a3, a4, a5, a6) EXTENDCOL((block_width + marginX), 3)
    }
    // Extending bottom rows
    pixel *pe, *pi, *pp;
    pe = dstE + (block_height - 1) * dstStride - marginX;
    pi = dstI + (block_height - 1) * dstStride - marginX;
    pp = dstP + (block_height - 1) * dstStride - marginX;
    for (int y = 1; y <= marginY; y++)
        memcpy(pe + y * dstStride, pe, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pi + y * dstStride, pi, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pp + y * dstStride, pp, block_width + marginX * 2);

    // Extending top rows
    pe -= ((block_height - 1) * dstStride);
    pi -= ((block_height - 1) * dstStride);
    pp -= ((block_height - 1) * dstStride);
    for (int y = 1; y <= marginY; y++)
        memcpy(pe - y * dstStride, pe, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pi - y * dstStride, pi, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pp - y * dstStride, pp, block_width + marginX * 2);
}

template<int N>
void filterVertical_p_p(int bitDepth,
                        pixel *src, int srcStride,
                        pixel *dst, int dstStride,
                        int block_width, int block_height,
                        short const *coeff)
{
    int row, col;
    int offset;
    short maxVal;
    int shift = IF_FILTER_PREC;

    src -= (N / 2 - 1) * srcStride;
    offset = 1 << (shift - 1);
    maxVal = (1 << bitDepth) - 1;

    Vec8s im0;
    im0.load(coeff);

    Vec8s cm[8];

    assert((N == 4) || (N == 8));
    cm[0] = broadcast(const_int(0), im0);
    cm[1] = broadcast(const_int(1), im0);
    cm[2] = broadcast(const_int(2), im0);
    cm[3] = broadcast(const_int(3), im0);

    if (N == 8)
    {
        cm[4] = broadcast(const_int(4), im0);
        cm[5] = broadcast(const_int(5), im0);
        cm[6] = broadcast(const_int(6), im0);
        cm[7] = broadcast(const_int(7), im0);
    }

    for (row = 0; row < block_height; row++)
    {
        for (col = 0; col < block_width - 15; col += 16)
        {
            Vec16uc row0, row1, row2, row3, row4, row5, row6, row7, sum;
            Vec8s row0_first, row0_last, row1_first, row1_last;
            Vec8s c0, c1, c2, c3, c4, c5, c6, c7;
            Vec8s  sum_first, sum_last;

            row0.load(&src[col]);
            row1.load(&src[col + srcStride]);

            c0 = cm[0];
            c1 = cm[1];

            row0_first = extend_low(row0);
            row1_first = extend_low(row1);
            row0_last  = extend_high(row0);
            row1_last  = extend_high(row1);

            row0_first = row0_first * c0;
            row1_first = row1_first * c1;
            row0_last = row0_last * c0;
            row1_last = row1_last * c1;

            sum_first = row0_first + row1_first;
            sum_last = row0_last + row1_last;

            row2.load(&src[col + 2 * srcStride]);
            row3.load(&src[col + 3 * srcStride]);

            c2 = cm[2];
            c3 = cm[3];

            row0_first = extend_low(row2);
            row0_last = extend_high(row2);
            row0_first = row0_first * c2;
            row0_last = row0_last * c2;
            row1_first = extend_low(row3);
            row1_last = extend_high(row3);
            row1_first = row1_first * c3;
            row1_last = row1_last * c3;
            sum_first += row0_first + row1_first;
            sum_last += row0_last + row1_last;

            if (N == 8)
            {
                row4.load(&src[col + 4 * srcStride]);
                row5.load(&src[col + 5 * srcStride]);

                c4 = cm[4];
                c5 = cm[5];

                row0_first = extend_low(row4);
                row0_last = extend_high(row4);
                row0_first = row0_first * c4;
                row0_last = row0_last * c4;
                row1_first = extend_low(row5);
                row1_last = extend_high(row5);
                row1_first = row1_first * c5;
                row1_last = row1_last * c5;
                sum_first += row0_first + row1_first;
                sum_last += row0_last + row1_last;

                row6.load(&src[col + 6 * srcStride]);
                row7.load(&src[col + 7 * srcStride]);

                c6 = cm[6];
                c7 = cm[7];

                row0_first = extend_low(row6);
                row0_last = extend_high(row6);
                row0_first = row0_first * c6;
                row0_last = row0_last * c6;
                row1_first = extend_low(row7);
                row1_last = extend_high(row7);
                row1_first = row1_first * c7;
                row1_last = row1_last * c7;

                sum_first += row0_first + row1_first;
                sum_last += row0_last + row1_last;
            }

            sum_first = (sum_first + offset)  >> shift;
            sum_last = (sum_last + offset)  >> shift;
            sum_first = max(sum_first, 0);
            sum_last = max(sum_last, 0);
            Vec8s maxVal_v(maxVal);
            sum_first = min(sum_first, maxVal_v);
            sum_last = min(sum_last, maxVal_v);

            sum = compress(sum_first, sum_last);

            sum.store(dst + col);
        }

        //Handle the case when block_width is not multiple of 16
        for (; col < block_width; col += 8)
        {
            Vec16uc row0, row1, row2, row3, row4, row5, row6, row7, sum;
            Vec8s row0_first, row0_last, row1_first, row1_last;
            Vec8s c0, c1, c2, c3, c4, c5, c6, c7;
            Vec8s  sum_first, sum_last;

            row0.load(&src[col]);
            row1.load(&src[col + srcStride]);

            c0 = cm[0];
            c1 = cm[1];

            row0_first = extend_low(row0);
            row1_first = extend_low(row1);
            row0_first = row0_first * c0;
            row1_first = row1_first * c1;

            sum_first = row0_first + row1_first;

            row2.load(&src[col + 2 * srcStride]);
            row3.load(&src[col + 3 * srcStride]);

            c2 = cm[2];
            c3 = cm[3];

            row0_first = extend_low(row2);
            row0_first = row0_first * c2;
            row1_first = extend_low(row3);
            row1_first = row1_first * c3;

            sum_first += row0_first + row1_first;

            if (N == 8)
            {
                row4.load(&src[col + 4 * srcStride]);
                row5.load(&src[col + 5 * srcStride]);

                c4 = cm[4];
                c5 = cm[5];

                row0_first = extend_low(row4);
                row0_first = row0_first * c4;
                row1_first = extend_low(row5);
                row1_first = row1_first * c5;
                sum_first += row0_first + row1_first;

                row6.load(&src[col + 6 * srcStride]);
                row7.load(&src[col + 7 * srcStride]);

                c6 = cm[6];
                c7 = cm[7];

                row0_first = extend_low(row6);
                row0_first = row0_first * c6;
                row1_first = extend_low(row7);
                row1_first = row1_first * c7;
                sum_first += row0_first + row1_first;
            }

            sum_first = (sum_first + offset)  >> shift;
            sum_first = max(sum_first, 0);
            Vec8s maxVal_v(maxVal);
            sum_first = min(sum_first, maxVal_v);
            sum = compress(sum_first, 0);
            sum.store_partial(block_width - col, dst + col);
        }

        src += srcStride;
        dst += dstStride;
    }
}

template<int N>
void filterHorizontal_p_p(int bitDepth,
                          pixel *src, int srcStride,
                          pixel *dst, int dstStride,
                          int block_width, int block_height,
                          short const *coeff)
{
    int row, col;
    int offset;
    short maxVal;
    int headRoom = IF_INTERNAL_PREC - bitDepth;
    offset =  (1 << (headRoom - 1));
    maxVal = (1 << bitDepth) - 1;
    src -= (N / 2 - 1);

    Vec8s vec_sum_low, vec_zero(0);
    Vec16uc vec_src0, vec_sum;
    Vec8s vec_c;
    vec_c.load(coeff);
    Vec8s vec_c0(coeff[0]), vec_c1(coeff[1]), vec_c2(coeff[2]), vec_c3(coeff[3]), vec_c4(coeff[4]), vec_c5(coeff[5]), vec_c6(coeff[6]), vec_c7(coeff[7]);
    Vec8s vec_offset(offset);
    Vec8s vec_maxVal(maxVal);
    for (row = 0; row < block_height; row++)
    {
        col = 0;
        for (; col < (block_width - 7); col += 8)               // Iterations multiple of 8
        {
            vec_src0.load(src + col);                           // Load the 8 elements
            vec_sum_low = extend_low(vec_src0) * vec_c0;        // Multiply by c[0]

            vec_src0.load(src + col + 1);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c1;       // Multiply by c[1]

            vec_src0.load(src + col + 2);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c2;       // Multiply by c[2]

            vec_src0.load(src + col + 3);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c3;       // Multiply by c[3]

            if (N == 8)
            {
                vec_src0.load(src + col + 4);                   // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c4;   // Multiply by c[4]

                vec_src0.load(src + col + 5);                   // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c5;   // Multiply by c[5]

                vec_src0.load(src + col + 6);                   // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c6;   // Multiply by c[6]

                vec_src0.load(src + col + 7);                   // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c7;   // Multiply by c[7]
            }

            vec_sum_low = (vec_sum_low + vec_offset);           // Add offset(value copied into all short vector elements) to sum_low
            vec_sum_low = vec_sum_low >> headRoom;
            vec_sum_low = max(vec_sum_low, 0);                  // (val < 0) ? 0 : val;
            vec_sum_low = min(vec_sum_low, vec_maxVal);         // (val > maxVal) ? maxVal : val;
            vec_sum = compress(vec_sum_low, vec_zero);          // Save two short vectors(Vec8s, Vec8s(0)) to single short vector(Vec8s)
            vec_sum.store_partial(8, dst + col);                // Store vector
        }

        for (; col < block_width; col++)                        // Remaining iterations
        {
            if (N == 8)
            {
                vec_src0.load(src + col);
            }
            else
            {
                vec_src0 = load_partial_by_i<4>(src + col);
            }
            // Assuming that there is no overflow (Everywhere in this function!)
            vec_sum_low = extend_low(vec_src0) * vec_c;
            int sum = horizontal_add(vec_sum_low);
            short val = (short)(sum + offset) >> headRoom;
            val = (val < 0) ? 0 : val;
            val = (val > maxVal) ? maxVal : val;
            dst[col] = (pixel)val;
        }

        src += srcStride;
        dst += dstStride;
    }
}

void filterHorizontalMultiplaneExtend(int /*bitDepth*/,
                                      pixel *src, int srcStride,
                                      short *intF, short* intA, short* intB, short* intC, int intStride,
                                      pixel *dstA, pixel *dstB, pixel *dstC, int dstStride,
                                      int block_width, int block_height,
                                      int marginX, int marginY)
{
    int row, col;

    src -= (8 / 2 - 1);
    int offset;
    int headRoom = IF_INTERNAL_PREC - 8;
    int shift = IF_FILTER_PREC;
    shift -= headRoom;
    offset = -IF_INTERNAL_OFFS << shift;

    __m128i vec_src0;
    __m128i vec_offset = _mm_set1_epi16(offset);
    __m128i sumaL, sumbL, sumcL, tmp, exp1;
    __m128i tmp16a, tmp16b, tmp16c;

    // Load Ai, ai += Ai*coefi      
    for (row = 0; row < block_height; row++)
    {
        col = 0;

            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col));
            sumbL = (_mm_unpacklo_epi8(vec_src0,_mm_setzero_si128()));
            sumbL = _mm_sub_epi16(_mm_setzero_si128(), sumbL);

            // a = b+=4*a1,  c+=1*a1
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 1));
            sumcL = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_add_epi16(sumbL, _mm_sll_epi16(sumcL,_mm_cvtsi32_si128(2)));
            sumaL = sumbL;
 
            // a +=-10*a2    b+=-11*a2      c+=-5*a2
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 2));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(-5));
            sumcL = _mm_add_epi16(sumcL, tmp);
            tmp = _mm_sll_epi16(tmp,_mm_cvtsi32_si128(1));
            sumaL = _mm_add_epi16(sumaL, tmp);
            sumbL = _mm_add_epi16(sumbL, tmp);


            // a +=58*a3    b+=40*a3      c+=17*a3
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 3));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            _mm_storeu_si128((__m128i*)(intF+col),_mm_sub_epi16(_mm_sll_epi16(tmp,_mm_cvtsi32_si128(6)), _mm_set1_epi16(IF_INTERNAL_OFFS)));
            exp1 = _mm_add_epi16(tmp, _mm_sll_epi16(tmp,_mm_cvtsi32_si128(4)));
            sumcL = _mm_add_epi16(sumcL, exp1);
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(40));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumaL = _mm_add_epi16(sumaL, _mm_add_epi16(exp1, tmp));
 
            // a +=17*a4    b+=40*a4      c+=58*a4
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 4));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            exp1 = _mm_add_epi16(tmp, _mm_sll_epi16(tmp,_mm_cvtsi32_si128(4)));
            sumaL = _mm_add_epi16(sumaL, exp1);
            sumcL = _mm_add_epi16(sumcL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(40));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumcL = _mm_add_epi16(sumcL, _mm_add_epi16(exp1, tmp));
 
            // a +=-5*a5    b+=-11*a5      c+=-10*a5
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 5));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(-5));
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp =_mm_sll_epi16(tmp,_mm_cvtsi32_si128(1));
            sumcL = _mm_add_epi16(sumcL, tmp);
            sumbL = _mm_add_epi16(sumbL, tmp);

            // a +=1*a6    b+=4*a6      c+=4*a6
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 6));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp = _mm_sll_epi16(tmp,_mm_cvtsi32_si128(2));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumcL = _mm_add_epi16(sumcL, tmp);

            // a +=0*a7    b+=-1*a7      c+=-1*a7
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 7));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            sumcL = _mm_sub_epi16(sumcL, tmp);
            sumaL = _mm_add_epi16(sumaL, vec_offset);
            sumbL = _mm_add_epi16(sumbL, vec_offset);
            sumcL = _mm_add_epi16(sumcL, vec_offset);

            _mm_storeu_si128((__m128i*)(intA+col),sumaL);
            sumaL = _mm_add_epi16(sumaL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumaL = _mm_sra_epi16(sumaL,_mm_cvtsi32_si128(6));
            tmp16a = _mm_packus_epi16(sumaL,sumaL);            
            _mm_storel_epi64((__m128i*)(dstA + row * dstStride + col),tmp16a);

            _mm_storeu_si128((__m128i*)(intB+col),sumbL);
            sumbL = _mm_add_epi16(sumbL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumbL = _mm_sra_epi16(sumbL,_mm_cvtsi32_si128(6));
            tmp16b = _mm_packus_epi16(sumbL,sumbL);            
            _mm_storel_epi64((__m128i*)(dstB + row * dstStride + col),tmp16b);

            _mm_storeu_si128((__m128i*)(intC+col),sumcL);
            sumcL = _mm_add_epi16(sumcL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumcL = _mm_sra_epi16(sumcL,_mm_cvtsi32_si128(6));
            tmp16c = _mm_packus_epi16(sumcL,sumcL);            
            _mm_storel_epi64((__m128i*)(dstC + row * dstStride + col),tmp16c);

            //Extend First column
        __m128i ma, mb, mc;
        ma = _mm_shuffle_epi8(tmp16a , _mm_set1_epi8(0)); 
        mb = _mm_shuffle_epi8(tmp16b , _mm_set1_epi8(0)); 
        mc = _mm_shuffle_epi8(tmp16c , _mm_set1_epi8(0)); 

        for (int i = -marginX; i < -16; i += 16) 
        { 
            _mm_storeu_si128((__m128i*)(dstA + row * dstStride +  i), ma);
            _mm_storeu_si128((__m128i*)(dstB + row * dstStride +  i), mb); 
            _mm_storeu_si128((__m128i*)(dstC + row * dstStride +  i), mc); 
        } 
        _mm_storeu_si128((__m128i*)(dstA + row * dstStride - 16), ma); /*Assuming marginX > 16*/ 
        _mm_storeu_si128((__m128i*)(dstB + row * dstStride - 16), mb); 
        _mm_storeu_si128((__m128i*)(dstC + row * dstStride - 16), mc); 

         col+=8;

        for (; col + 8/*16*/ <= (block_width); col += 8/*16*/)               // Iterations multiple of 8
        {
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col));
            sumbL = (_mm_unpacklo_epi8(vec_src0,_mm_setzero_si128()));
            sumbL = _mm_sub_epi16(_mm_setzero_si128(), sumbL);

            // a = b+=4*a1,  c+=1*a1
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 1));
            sumcL = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_add_epi16(sumbL, _mm_sll_epi16(sumcL,_mm_cvtsi32_si128(2)));
            sumaL = sumbL;
 
            // a +=-10*a2    b+=-11*a2      c+=-5*a2
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 2));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(-5));
            sumcL = _mm_add_epi16(sumcL, tmp);
            tmp = _mm_sll_epi16(tmp,_mm_cvtsi32_si128(1));
            sumaL = _mm_add_epi16(sumaL, tmp);
            sumbL = _mm_add_epi16(sumbL, tmp);


            // a +=58*a3    b+=40*a3      c+=17*a3
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 3));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            _mm_storeu_si128((__m128i*)(intF+col),_mm_sub_epi16(_mm_sll_epi16(tmp,_mm_cvtsi32_si128(6)), _mm_set1_epi16(IF_INTERNAL_OFFS)));
            exp1 = _mm_add_epi16(tmp, _mm_sll_epi16(tmp,_mm_cvtsi32_si128(4)));
            sumcL = _mm_add_epi16(sumcL, exp1);
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(40));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumaL = _mm_add_epi16(sumaL, _mm_add_epi16(exp1, tmp));
 
            // a +=17*a4    b+=40*a4      c+=58*a4
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 4));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            exp1 = _mm_add_epi16(tmp, _mm_sll_epi16(tmp,_mm_cvtsi32_si128(4)));
            sumaL = _mm_add_epi16(sumaL, exp1);
            sumcL = _mm_add_epi16(sumcL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(40));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumcL = _mm_add_epi16(sumcL, _mm_add_epi16(exp1, tmp));
 
            // a +=-5*a5    b+=-11*a5      c+=-10*a5
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 5));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            tmp = _mm_mullo_epi16(tmp, _mm_set1_epi16(-5));
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp =_mm_sll_epi16(tmp,_mm_cvtsi32_si128(1));
            sumcL = _mm_add_epi16(sumcL, tmp);
            sumbL = _mm_add_epi16(sumbL, tmp);

            // a +=1*a6    b+=4*a6      c+=4*a6
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 6));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumaL = _mm_add_epi16(sumaL, tmp);
            tmp = _mm_sll_epi16(tmp,_mm_cvtsi32_si128(2));
            sumbL = _mm_add_epi16(sumbL, tmp);
            sumcL = _mm_add_epi16(sumcL, tmp);

            // a +=0*a7    b+=-1*a7      c+=-1*a7
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + col + 7));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            sumbL = _mm_sub_epi16(sumbL, tmp);
            sumcL = _mm_sub_epi16(sumcL, tmp);
            sumaL = _mm_add_epi16(sumaL, vec_offset);
            sumbL = _mm_add_epi16(sumbL, vec_offset);
            sumcL = _mm_add_epi16(sumcL, vec_offset);

            _mm_storeu_si128((__m128i*)(intA+col),sumaL);
            sumaL = _mm_add_epi16(sumaL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumaL = _mm_sra_epi16(sumaL,_mm_cvtsi32_si128(6));
            tmp16a = _mm_packus_epi16(sumaL,sumaL);            
            _mm_storel_epi64((__m128i*)(dstA + row * dstStride + col),tmp16a);

            _mm_storeu_si128((__m128i*)(intB+col),sumbL);
            sumbL = _mm_add_epi16(sumbL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumbL = _mm_sra_epi16(sumbL,_mm_cvtsi32_si128(6));
            tmp16b = _mm_packus_epi16(sumbL,sumbL);            
            _mm_storel_epi64((__m128i*)(dstB + row * dstStride + col),tmp16b);

            _mm_storeu_si128((__m128i*)(intC+col),sumcL);
            sumcL = _mm_add_epi16(sumcL, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
            sumcL = _mm_sra_epi16(sumcL,_mm_cvtsi32_si128(6));
            tmp16c = _mm_packus_epi16(sumcL,sumcL);            
            _mm_storel_epi64((__m128i*)(dstC + row * dstStride + col),tmp16c);

        }

        if (block_width - col > 0)
        {
            vec_src0 = _mm_loadu_si128((__m128i const*)(src + block_width - 5));
            tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());
            _mm_storeu_si128((__m128i*)(intF + block_width - 8),_mm_sub_epi16(_mm_sll_epi16(tmp,_mm_cvtsi32_si128(6)), _mm_set1_epi16(IF_INTERNAL_OFFS)));
            __m128i a, b, c, sum1, sum2, sum3=_mm_setzero_si128();
            for (; col < block_width; col++)                           // Remaining iterations
            {
                vec_src0 = _mm_loadu_si128((__m128i const*)(src + col));
                tmp = _mm_unpacklo_epi8(vec_src0,_mm_setzero_si128());    // Assuming that there is no overflow (Everywhere in this function!)
                a = _mm_setr_epi16(-1, 4, -10, 58, 17,  -5, 1,  0);
                a = _mm_mullo_epi16(tmp, a);
                b = _mm_setr_epi16(-1, 4, -11, 40, 40, -11, 4, -1);
                b = _mm_mullo_epi16(tmp, b);
                c = _mm_setr_epi16(0, 1,  -5, 17, 58, -10, 4, -1);
                c = _mm_mullo_epi16(tmp, c);
                sum1  = _mm_hadd_epi16(a,b);                   // horizontally add 8 elements in 3 steps
                sum2  = _mm_hadd_epi16(c,c);
                sum2  = _mm_hadd_epi16(sum1,sum2);
                sum3  = _mm_hadd_epi16(sum2,sum2);
                sum3  = _mm_add_epi16(sum3, vec_offset);
                sum3  = _mm_sra_epi16(sum3,_mm_cvtsi32_si128(shift));
                intA[col] = _mm_cvtsi128_si32(sum3);  
                intB[col] = _mm_extract_epi16(sum3, 1);
                intC[col] = _mm_extract_epi16(sum3, 2);
                sum3 = _mm_add_epi16(sum3, _mm_set1_epi16(IF_INTERNAL_OFFS + 32));
                sum3 = _mm_sra_epi16(sum3,_mm_cvtsi32_si128(6));
                sum3 = _mm_packus_epi16(sum3, sum3);
                dstA[row * dstStride + col] = _mm_extract_epi8(sum3, 0);
                dstB[row * dstStride + col] = _mm_extract_epi8(sum3, 1);
                dstC[row * dstStride + col] = _mm_extract_epi8(sum3, 2);
            }
            tmp16a = _mm_shuffle_epi8(sum3 , _mm_set1_epi8(0)); 
            tmp16b = _mm_shuffle_epi8(sum3 , _mm_set1_epi8(1)); 
            tmp16c = _mm_shuffle_epi8(sum3 , _mm_set1_epi8(2)); 
        }
        else
        {
            tmp16a = _mm_shuffle_epi8(tmp16a , _mm_set1_epi8(15)); 
            tmp16b = _mm_shuffle_epi8(tmp16b , _mm_set1_epi8(15)); 
            tmp16c = _mm_shuffle_epi8(tmp16c , _mm_set1_epi8(15)); 
        }
        //Extend last column
        for (int i = -marginX; i < -16; i += 16) 
        { 
            _mm_storeu_si128((__m128i*)(dstA + row * dstStride + block_width + marginX + i), tmp16a);
            _mm_storeu_si128((__m128i*)(dstB + row * dstStride + block_width + marginX + i), tmp16b); 
            _mm_storeu_si128((__m128i*)(dstC + row * dstStride + block_width + marginX + i), tmp16c); 
        } 
        _mm_storeu_si128((__m128i*)(dstA + row * dstStride + block_width + marginX - 16), tmp16a); /*Assuming marginX > 16*/ 
        _mm_storeu_si128((__m128i*)(dstB + row * dstStride + block_width + marginX - 16), tmp16b); 
        _mm_storeu_si128((__m128i*)(dstC + row * dstStride + block_width + marginX - 16), tmp16c); 

        src += srcStride;
        intF += intStride;
        intA += intStride;
        intB += intStride;
        intC += intStride;
    }

    // Extending bottom rows
    pixel *pe, *pi, *pp;
    pe = dstA + (block_height - 1) * dstStride - marginX;
    pi = dstB + (block_height - 1) * dstStride - marginX;
    pp = dstC + (block_height - 1) * dstStride - marginX;
    for (int y = 1; y <= marginY; y++)
        memcpy(pe + y * dstStride, pe, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pi + y * dstStride, pi, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pp + y * dstStride, pp, block_width + marginX * 2);

    // Extending top rows
    pe  = dstA - marginX;
    pi  = dstB - marginX;
    pp  = dstC - marginX;
    for (int y = 1; y <= marginY; y++)
        memcpy(pe - y * dstStride, pe, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pi - y * dstStride, pi, block_width + marginX * 2);
    for (int y = 1; y <= marginY; y++)
        memcpy(pp - y * dstStride, pp, block_width + marginX * 2);
}

template<int N>
void filterHorizontal_p_s(int bitDepth, pixel *src, int srcStride, short *dst, int dstStride, int block_width, int block_height, short const *coeff)
{
    int row, col;

    src -= (N / 2 - 1);
    int offset;
    int headRoom = IF_INTERNAL_PREC - bitDepth;
    int shift = IF_FILTER_PREC;
    shift -= headRoom;
    offset = -IF_INTERNAL_OFFS << shift;

    Vec8s vec_sum_low, vec_sum_high;
    Vec16uc vec_src0, vec_sum;
    Vec8s vec_c;
    vec_c.load(coeff);
    Vec8s vec_c0(coeff[0]), vec_c1(coeff[1]), vec_c2(coeff[2]), vec_c3(coeff[3]), vec_c4(coeff[4]), vec_c5(coeff[5]), vec_c6(
        coeff[6]), vec_c7(coeff[7]);
    Vec8s vec_offset(offset);

    for (row = 0; row < block_height; row++)
    {
        col = 0;
        for (; col < (block_width - 7); col += 8)               // Iterations multiple of 8
        {
            vec_src0.load(src + col);                           // Load the 8 elements
            vec_sum_low = extend_low(vec_src0) * vec_c0;        // Multiply by c[0]

            vec_src0.load(src + col + 1);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c1;       // Multiply by c[1]

            vec_src0.load(src + col + 2);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c2;       // Multiply by c[2]

            vec_src0.load(src + col + 3);                       // Load the 8 elements
            vec_sum_low += extend_low(vec_src0) * vec_c3;       // Multiply by c[3]

            if (N == 8)
            {
                vec_src0.load(src + col + 4);                     // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c4;     // Multiply by c[4]

                vec_src0.load(src + col + 5);                     // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c5;     // Multiply by c[5]

                vec_src0.load(src + col + 6);                     // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c6;     // Multiply by c[6]

                vec_src0.load(src + col + 7);                     // Load the 8/16 elements
                vec_sum_low += extend_low(vec_src0) * vec_c7;     // Multiply by c[7]
            }
            vec_sum_low = (vec_sum_low + vec_offset);               // Add offset(value copied into all integer vector elements) to sum_low
            vec_sum_low = vec_sum_low >> shift;
            vec_sum_low.store(dst + col);                           // Store vector
        }

        for (; col < block_width; col++)                           // Remaining iterations
        {
            if (N == 8)
            {
                vec_src0.load(src + col);
            }
            else
            {
                vec_src0 = load_partial_by_i<4>(src + col);
            }
            vec_sum_low = extend_low(vec_src0) * vec_c;                        // Assuming that there is no overflow (Everywhere in this function!)
            int sum = horizontal_add(vec_sum_low);
            short val = (short)(sum + offset) >> shift;
            dst[col] = val;
        }

        src += srcStride;
        dst += dstStride;
    }
}

void filterConvertPelToShort(int bitDepth, pixel *src, int srcStride,
                             short *dst, int dstStride, int width, int height)
{
    pixel* srcOrg = src;
    short* dstOrg = dst;
    int shift = IF_INTERNAL_PREC - bitDepth;
    int row, col;
    Vec16uc src_v;
    Vec8s dst_v, val_v;

    for (row = 0; row < height; row++)
    {
        for (col = 0; col < width - 7; col += 8)
        {
            src_v.load(src + col);
            val_v = extend_low(src_v) << shift;
            dst_v = val_v - IF_INTERNAL_OFFS;
            dst_v.store(dst + col);
        }

        src += srcStride;
        dst += dstStride;
    }

    if (width % 8 != 0)
    {
        src = srcOrg;
        dst = dstOrg;
        col = width - (width % 8);
        for (row = 0; row < height; row++)
        {
            src_v.load(src + col);
            val_v = extend_low(src_v) << shift;
            dst_v = val_v - IF_INTERNAL_OFFS;
            dst_v.store_partial(width - col, dst + col);
            src += srcStride;
            dst += dstStride;
        }
    }
}

void filterConvertShortToPel(int bitDepth, short *src, int srcStride,
                             pixel *dst, int dstStride, int width, int height)
{
    short* srcOrg = src;
    pixel* dstOrg = dst;
    int shift = IF_INTERNAL_PREC - bitDepth;
    short offset = IF_INTERNAL_OFFS;

    offset += shift ? (1 << (shift - 1)) : 0;
    short maxVal = (1 << bitDepth) - 1;
    Vec8s minVal(0);
    int row, col;
    Vec8s src_c, val_c, val_zero(0);
    Vec16uc val_uc;
    for (row = 0; row < height; row++)
    {
        for (col = 0; col < width - 7; col += 8)
        {
            src_c.load(src + col);
            val_c = add_saturated(src_c, offset) >> shift;
            val_c = max(val_c, minVal);
            val_c = min(val_c, maxVal);
            val_uc = compress(val_c, val_zero);
            val_uc.store_partial(8, dst + col);
        }

        src += srcStride;
        dst += dstStride;
    }

    if (width % 8 != 0)
    {
        src = srcOrg;
        dst = dstOrg;
        col = width - (width % 8);
        for (row = 0; row < height; row++)
        {
            src_c.load(src + col);
            val_c = add_saturated(src_c, offset) >> shift;
            val_c = max(val_c, minVal);
            val_c = min(val_c, maxVal);
            val_uc = compress(val_c, val_zero);
            val_uc.store_partial(width - col, dst + col);
            src += srcStride;
            dst += dstStride;
        }
    }
}
